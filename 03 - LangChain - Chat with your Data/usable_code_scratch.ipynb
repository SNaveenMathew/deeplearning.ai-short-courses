{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e09aea-39b4-4a45-961c-8c57972012e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501c5bbd-1b6b-47a5-aa6a-964a9f58add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "loader = NotionDirectoryLoader(\"../../tech_non_tech_blog/_posts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927c21cc-4a22-48ae-b44b-8c244270f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "567f0be9-482d-4e9c-a53b-6de0c1aaae26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '..\\\\..\\\\tech_non_tech_blog\\\\_posts\\\\2019-02-02-Speeding-up-diff-between-consecutive-rows-in-python-on-my-laptop.md'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d2bb48-8f54-4fc1-8382-6b11c35d9d82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "\n",
    "def extract_txt(docs):\n",
    "    txt = ' '.join([d.page_content for d in docs])\n",
    "    return txt\n",
    "\n",
    "md_header_splits = markdown_splitter.split_text(extract_txt(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0986800-2794-4d85-9fe8-8376011d2977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"I'm currently pursuing independent research on large financial data. Data engineering is an integral part of the analysis as financial data is often not in the format required for analysis. Financial data may be provided in the form of transactions/updates or in the form of aggregates. While it is easy to go from transaction level to aggregate level, it is difficult to do the opposite.  \\nMy laptop configuration is decent: Ubuntu 18.04, 16 GB DDR4, Intel core i7–8750H (6 + 6 virtual core) @ 2.2 GHz and a GPU (not relevant here). But brute force is not a good idea — code run time was ~ 4 hours for processing one day's data, which is not acceptable.\", metadata={'Header 2': \"Speeding-up 'diff' between Consecutive Rows in Python on My Laptop\", 'Header 3': 'Introduction'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564af4a4-f7ec-47ac-8572-e24d10019572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lessons from my Internship (and Immediate Aftermath)                              8\n",
       "Search for Exoplanets - Humans vs. Stars                                          8\n",
       "Speeding-up 'diff' between Consecutive Rows in Python on My Laptop                6\n",
       "Does python prioritize True in 'OR' and False in 'AND' condition checks?          6\n",
       "Unsupervised Deep Learning in Astronomy for Exoplanet Candidate Identification    5\n",
       "Running Python or R in Android OS                                                 5\n",
       "                                                                                  1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series([md_header_split.metadata.get('Header 2', '') for md_header_split in md_header_splits]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9311c0-8107-4c88-a659-2ab2cd948abf",
   "metadata": {},
   "source": [
    "To fix the chunking issue the `---` header section had to be added to each .md file. Now things look better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1356f553-8513-4726-854a-3b5b8ffa5531",
   "metadata": {},
   "source": [
    "## Finding similarity using SpacyEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "addb32ac-ef71-4d6f-8c84-908d2a19a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "embedding = SpacyEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fda25ae-e43e-45bc-b55e-6baae24d110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding.embed_query(\"What's my laptop configuration?\")\n",
    "embedding2s = [embedding.embed_query(md_header_split.page_content) for md_header_split in md_header_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f542d501-e8b2-447c-a0b8-ba00b1d137ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "similarities = [np.dot(embedding2, embedding1) for embedding2 in embedding2s]\n",
    "np.argmax(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25995398-33fc-4469-807b-9c7cc4dade24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"I'm currently pursuing independent research on large financial data. Data engineering is an integral part of the analysis as financial data is often not in the format required for analysis. Financial data may be provided in the form of transactions/updates or in the form of aggregates. While it is easy to go from transaction level to aggregate level, it is difficult to do the opposite.  \\nMy laptop configuration is decent: Ubuntu 18.04, 16 GB DDR4, Intel core i7–8750H (6 + 6 virtual core) @ 2.2 GHz and a GPU (not relevant here). But brute force is not a good idea — code run time was ~ 4 hours for processing one day's data, which is not acceptable.\",\n",
       "       '---\\nlayout: post\\ndate: 2019-01-11 12:00:00 -0500\\nimage: ../data/multiprocessing.jpg\\n---',\n",
       "       \"I recently extended the analysis, but there are several more ideas for 'discretization'.\\n---\\nlayout: post\\ndate: 2019-09-21 12:00:00 -0500\\nimage: ../data/Transit_photometry.gif\\n---\",\n",
       "       \"1.Follow the instructions in [this README] (https://github.com/Neo-Oli/termux-ubuntu/blob/master/README.md)\\n2. Run ./start-ubuntu. You're now a root user\\n3. Follow the normal steps used to install R on Ubuntu (similar steps for Python)  \\nI'm not sure if F-Droid can be uninstalled after this step. I will update this article once I'm in a position to experiment.\",\n",
       "       \"Summer internship can be tiring, but I feel the opposite. In the end, the ‘result’ is a binary flag which currently has a value False. This is one of the short run motivations. The long run motivation is philosophical. I rejected 2 (higher paying) offers to signup for a dream — to accomplish something significant in life. Dreams don't come true in 3 months, but there were significant learnings during Summer.\"],\n",
       "      dtype='<U2192')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "top_k = np.argsort(-np.array(similarities))[0:k]\n",
    "np.array([md_header_split.page_content for md_header_split in md_header_splits])[top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed55b33-b5c8-443d-9e8c-b62289afa868",
   "metadata": {},
   "source": [
    "## Storing in vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "690e7989-1de5-4b40-aeef-ced57c16a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'docs/tech_non_tech_blog/'# Better to use a new folder here. There's a chromadb version compatibility issue while appending to the existing docs/chromadb folder\n",
    "vectordb = Chroma.from_documents(docs, embedding, persist_directory=persist_directory)\n",
    "if len(vectordb.get()['documents']) == 0:\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=md_header_splits,\n",
    "        embedding=embedding,# Reusing the spacy embedding\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc069e05-4108-4201-89fe-8116d83a39cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm currently pursuing independent research on large financial data. Data engineering is an integral part of the analysis as financial data is often not in the format required for analysis. Financial data may be provided in the form of transactions/updates or in the form of aggregates. While it is easy to go from transaction level to aggregate level, it is difficult to do the opposite.  \\nMy laptop configuration is decent: Ubuntu 18.04, 16 GB DDR4, Intel core i7–8750H (6 + 6 virtual core) @ 2.2 GHz and a GPU (not relevant here). But brute force is not a good idea — code run time was ~ 4 hours for processing one day's data, which is not acceptable.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What's my laptop configuration?\"\n",
    "ret_docs = vectordb.similarity_search(question,k=3)\n",
    "ret_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00130522-9df5-4f90-aa67-9277db502f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. Brushing, zooming and refining are time consuming tasks. They are needed because the current set of methods are not robust to 'noisy tagging' — for example: terminating the task after brushing (without zooming and refining) leads to light curve of candidate + default light curve of the host star.\\n2. Can this be simplified further? Sections of the image can be automatically brushing (using unsupervised learning) and labeled as candidates. This process can be repeated for all light curves. It is alright to have few false positives, but the algorithm should produce much fewer false negatives. Manual taggers only have to click on the 'x' button on each brush or leave the light curve unaltered (approval that the automatic candidate identification is accurate). Brushing, zooming and refining will still be available to taggers, but the hope here is to reduce their usage.  \\n- If a light curve has 'large' number of taggers, their approval and disapproval (+ optional refining) can be used to denoise the zone in which the candidate is present\\n- If the light curve has 'small' number of taggers, the default set of candidates tagged by the algorithm may be considered  \\nTherefore, unsupervised learning can expedite the process of candidate identification. Eventually each 'zone' of the light curve and each user who tags will be given a credibility score. This score varies with each newly available example of 'modified' manual tagging. Eventually, the most credible candidates can be examined further or can be used to build a supervised learning model.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How long did brute force run for?\"\n",
    "ret_docs = vectordb.similarity_search(question,k=3)\n",
    "ret_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de0c3d05-3e92-4453-914f-65a061bd2e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\'s possible to setup Ubuntu like windowing system for Ubuntu in Android OS. Once again, some people claimed to be successful using the steps in this [Reddit post](https://www.reddit.com/r/termux/comments/184kb1c/this_is_just_a_quick_rundown_of_termuxx11/). I tried this, and also tried installing x11 using `apt install`, but neither solution worked for me. So, I\\'ll add another disclaimer - \"Your results may vary\", but I hit a dead-end.  \\nInstead of setting up a windowing system like x11, I ran the RShiny app using `Rscript` command - this doesn\\'t pop-up a new window with the app, but the app can be opened in any browser by entering \"localhost:<port_number>\". To avoid randomness in the port number it\\'s  better to run `Rscript -e \"shiny::runApp(host=\\'127.0.0.1\\', port=<any_number_of_your_choice>)\"`.  \\nNow we have R (or Python) running in Android OS. Tablet hardware (especially CPUs) are meant to conserve energy, so they may be extremely slow with compilation and program execution (as you might\\'ve noticed if you followed along with the installation of tools/software like gcc in Termux-Ubuntu). It\\'s time to tackle some interesting data-driven problems (I\\'m pursuing one - again, it\\'s not for everyone; more on this soon).'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_docs[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45e30cbe-608b-4e85-bb5c-e5abbf264d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='If the transition probabilities are known, a model free approximation of the Q function can be computed based on historical data. The agent can be made to act in a greedy way (immediate reward, without exploration) to achieve its goal. However, it should be noted that a set of locally optimal paths is not guaranteed to build a globally optimal path. Therefore, this method is not guaranteed to work in environments that are drastically different from those observed in the past.  \\nIf the transition probabilities are known, a model free approximation of the Q function can be computed based on historical data. The agent can be made to act in a greedy way (immediate reward, without exploration) to achieve its goal. However, it should be noted that a set of locally optimal paths is not guaranteed to build a globally optimal path. Therefore, this method is not guaranteed to work in environments that are drastically different from those observed in the past.', metadata={'Header 2': 'Lessons from my Internship (and Immediate Aftermath)', 'Header 3': 'Simpler Solution'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)\n",
    "docs_mmr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd3139f0-f99f-4a64-8199-4b5c502162ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='#### Problem  \\nThe result is a pandas series of lists. But my desired output is a structure with timestamp + edit in each row. So I used pd.concat(result), which had an additional run time of 3.5 mins (~ clock time for consecutive diff which is a relatively complicated process). This drove me nuts!  \\n#### Solution  \\nI found that np.concatenate is much faster (~ 40 sec) because of homogeneity. For simplicity I converted timestamp and all col_idx_values to string. The output was written to a CSV file using np.savetxt(\"file.csv\", concat_result, fmt=\"%s\") or using array_name.tofile(\"file.csv\", sep=\",\", format=\"%s\").', metadata={'Header 2': \"Speeding-up 'diff' between Consecutive Rows in Python on My Laptop\", 'Header 3': 'Concatenation — An Additional Problem'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How long did brute force run for?\"\n",
    "ret_docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={\"Header 2\":\"Speeding-up 'diff' between Consecutive Rows in Python on My Laptop\"}\n",
    ")\n",
    "ret_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1875385-ae0d-416f-b2e9-7cbd07e77433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"1. Brushing, zooming and refining are time consuming tasks. They are needed because the current set of methods are not robust to 'noisy tagging' — for example: terminating the task after brushing (without zooming and refining) leads to light curve of candidate + default light curve of the host star.\\n2. Can this be simplified further? Sections of the image can be automatically brushing (using unsupervised learning) and labeled as candidates. This process can be repeated for all light curves. It is alright to have few false positives, but the algorithm should produce much fewer false negatives. Manual taggers only have to click on the 'x' button on each brush or leave the light curve unaltered (approval that the automatic candidate identification is accurate). Brushing, zooming and refining will still be available to taggers, but the hope here is to reduce their usage.  \\n- If a light curve has 'large' number of taggers, their approval and disapproval (+ optional refining) can be used to denoise the zone in which the candidate is present\\n- If the light curve has 'small' number of taggers, the default set of candidates tagged by the algorithm may be considered  \\nTherefore, unsupervised learning can expedite the process of candidate identification. Eventually each 'zone' of the light curve and each user who tags will be given a credibility score. This score varies with each newly available example of 'modified' manual tagging. Eventually, the most credible candidates can be examined further or can be used to build a supervised learning model.\", metadata={'Header 2': 'Search for Exoplanets - Humans vs. Stars', 'Header 3': 'Can this task be simplified?'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How long did brute force run for?\"\n",
    "ret_docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={\"Header 2\":\"Search for Exoplanets - Humans vs. Stars\"}\n",
    ")\n",
    "ret_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5cedd6-ce20-4616-a317-c227a037a294",
   "metadata": {},
   "source": [
    "### Other types of retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0de65c80-86aa-4955-a151-4f5f5e39344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "svm_retriever = SVMRetriever.from_texts([md_header_split.page_content for md_header_split in md_header_splits], embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8541d02a-410b-4436-975f-e1bd3d7870ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='If the transition probabilities are known, a model free approximation of the Q function can be computed based on historical data. The agent can be made to act in a greedy way (immediate reward, without exploration) to achieve its goal. However, it should be noted that a set of locally optimal paths is not guaranteed to build a globally optimal path. Therefore, this method is not guaranteed to work in environments that are drastically different from those observed in the past.  \\nIf the transition probabilities are known, a model free approximation of the Q function can be computed based on historical data. The agent can be made to act in a greedy way (immediate reward, without exploration) to achieve its goal. However, it should be noted that a set of locally optimal paths is not guaranteed to build a globally optimal path. Therefore, this method is not guaranteed to work in environments that are drastically different from those observed in the past.')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How long did brute force run for?\"\n",
    "docs_svm=svm_retriever.get_relevant_documents(question)\n",
    "docs_svm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1df1d9c9-b013-461d-90ec-b0654489f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_retriever = TFIDFRetriever.from_texts([md_header_split.page_content for md_header_split in md_header_splits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dcb62dc-1389-4cec-a5a7-92aad5ee2b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Disclaimer: If one has the option, one should use rstudio.cloud and shinyapps.io to develop and deploy RShiny apps. Following this article is a waste of time.  \\nWorking on R/Python in Android OS may sound like an artificial constraint, but I faced this scenario - I wanted to run a RShiny app, but I did not have the resources to run apps in the 'free tier' both locally and on the Cloud.  \\nExperts claim Android OS is just a version of Linux, but I'm not an expert in this area. This short article covers how a layman (like me) can run R scripts in Android OS. Similar steps can be followed to install and run Python in Android OS. Clearly, this is not an everyday scenario; not even something worth trying for fun.\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_tfidf=tfidf_retriever.get_relevant_documents(question)\n",
    "docs_tfidf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df528c7e-73d4-41fa-b3d0-f99deb99da3c",
   "metadata": {},
   "source": [
    "## Question answering\n",
    "\n",
    "Note: For now this section of code won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32c1bfcc-d817-4a34-a7ae-1a1e987653c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = open(\"../OPENAI_API_KEY.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76c55a7c-da11-4744-b25c-71033b97e612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "current_date = datetime.datetime.now().date()\n",
    "if current_date < datetime.date(2023, 9, 2):\n",
    "    llm_name = \"gpt-3.5-turbo-0301\"\n",
    "else:\n",
    "    llm_name = \"gpt-3.5-turbo\"\n",
    "print(llm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c973410-8d9b-4775-9c94-eafafb9ae4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0694cf0-a4a1-4254-bdc7-c97bf1a8fa05",
   "metadata": {},
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60141ec3-6495-4331-bf2a-0544badf0b63",
   "metadata": {},
   "source": [
    "### Using langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd4e308e-82bc-4859-95f4-306fa42c1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = open(\"../LANGCHAIN_API_KEY.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "510405d1-36ad-4b8d-84c2-0dd45c673086",
   "metadata": {},
   "source": [
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"refine\"# can use \"map_reduce\" or \"refine\"\n",
    ")\n",
    "result = qa_chain_mr({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d69afe-96ca-4ff5-92f1-e2e4af2b9305",
   "metadata": {},
   "source": [
    "## Chat\n",
    "\n",
    "Note: For now this section of code won't work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d8225-68ba-4fb4-a904-ac3f12e2cf21",
   "metadata": {},
   "source": [
    "### We've already seen this"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d194047f-c7f3-4155-87e0-97c572413f1b",
   "metadata": {},
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d554d2-da1b-4ad8-a987-db9892c17a4a",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eede93bd-29d4-4dfb-adb5-c3c9f82b69b4",
   "metadata": {},
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "retriever=vectordb.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "result = qa({\"question\": question})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d656fdb2-3aa7-4d43-a3ed-2116ecd52d58",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b35e0678-0d34-489f-ba26-7aa1ff6475ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ef7ebd5-d9dc-4d44-8b45-1caf98a480a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(file, chain_type, k):\n",
    "    # load documents\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "    # split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    # define embedding\n",
    "    # embeddings = OpenAIEmbeddings()\n",
    "    embeddings = SpacyEmbeddings()\n",
    "    # create vector database from data\n",
    "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "    # define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    # create a chatbot chain. Memory is managed externally.\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=llm_name, temperature=0), \n",
    "        chain_type=chain_type, \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "    )\n",
    "    return qa \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7787994-7734-438f-b5fb-5e55db5b7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    chat_history = param.List([])\n",
    "    answer = param.String(\"\")\n",
    "    db_query  = param.String(\"\")\n",
    "    db_response = param.List([])\n",
    "    \n",
    "    def __init__(self,  **params):\n",
    "        super(cbfs, self).__init__( **params)\n",
    "        self.panels = []\n",
    "        self.loaded_file = \"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"\n",
    "        self.qa = load_db(self.loaded_file,\"stuff\", 4)\n",
    "    \n",
    "    def call_load_db(self, count):\n",
    "        if count == 0 or file_input.value is None:  # init or no file specified :\n",
    "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "        else:\n",
    "            file_input.save(\"temp.pdf\")  # local copy\n",
    "            self.loaded_file = file_input.filename\n",
    "            button_load.button_style=\"outline\"\n",
    "            self.qa = load_db(\"temp.pdf\", \"stuff\", 4)\n",
    "            button_load.button_style=\"solid\"\n",
    "        self.clr_history()\n",
    "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "\n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
    "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
    "        self.chat_history.extend([(query, result[\"answer\"])])\n",
    "        self.db_query = result[\"generated_question\"]\n",
    "        self.db_response = result[\"source_documents\"]\n",
    "        self.answer = result['answer'] \n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))\n",
    "        ])\n",
    "        inp.value = ''  #clears loading indicator when cleared\n",
    "        return pn.WidgetBox(*self.panels,scroll=True)\n",
    "\n",
    "    @param.depends('db_query ', )\n",
    "    def get_lquest(self):\n",
    "        if not self.db_query :\n",
    "            return pn.Column(\n",
    "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n",
    "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
    "            )\n",
    "        return pn.Column(\n",
    "            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n",
    "            pn.pane.Str(self.db_query )\n",
    "        )\n",
    "\n",
    "    @param.depends('db_response', )\n",
    "    def get_sources(self):\n",
    "        if not self.db_response:\n",
    "            return \n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for doc in self.db_response:\n",
    "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    @param.depends('convchain', 'clr_history') \n",
    "    def get_chats(self):\n",
    "        if not self.chat_history:\n",
    "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for exchange in self.chat_history:\n",
    "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "727bd3eb-8ffd-47b5-aef7-905e329f4a43",
   "metadata": {},
   "source": [
    "cb = cbfs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dlai-short-courses)",
   "language": "python",
   "name": "dlai-short-courses"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
