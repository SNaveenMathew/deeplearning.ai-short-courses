{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e09aea-39b4-4a45-961c-8c57972012e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import openai\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501c5bbd-1b6b-47a5-aa6a-964a9f58add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "loader = NotionDirectoryLoader(\"../../tech_non_tech_blog/_posts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927c21cc-4a22-48ae-b44b-8c244270f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "567f0be9-482d-4e9c-a53b-6de0c1aaae26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '..\\\\..\\\\tech_non_tech_blog\\\\_posts\\\\2019-02-02-Speeding-up-diff-between-consecutive-rows-in-python-on-my-laptop.md'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d2bb48-8f54-4fc1-8382-6b11c35d9d82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"I'm currently pursuing independent research on large financial data. Data engineering is an integral part of the analysis as financial data is often not in the format required for analysis. Financial data may be provided in the form of transactions/updates or in the form of aggregates. While it is easy to go from transaction level to aggregate level, it is difficult to do the opposite.  \\nMy laptop configuration is decent: Ubuntu 18.04, 16 GB DDR4, Intel core i7–8750H (6 + 6 virtual core) @ 2.2 GHz and a GPU (not relevant here). But brute force is not a good idea — code run time was ~ 4 hours for processing one day's data, which is not acceptable.\", metadata={'Header 2': \"Speeding-up 'diff' between Consecutive Rows in Python on My Laptop\", 'Header 3': 'Introduction'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "\n",
    "def extract_txt(docs):\n",
    "    txt = ' '.join([d.page_content for d in docs])\n",
    "    return txt\n",
    "\n",
    "md_header_splits = markdown_splitter.split_text(extract_txt(docs))\n",
    "md_header_splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "564af4a4-f7ec-47ac-8572-e24d10019572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lessons from my Internship (and Immediate Aftermath)                              8\n",
       "Search for Exoplanets - Humans vs. Stars                                          8\n",
       "Speeding-up 'diff' between Consecutive Rows in Python on My Laptop                6\n",
       "Does python prioritize True in 'OR' and False in 'AND' condition checks?          6\n",
       "Unsupervised Deep Learning in Astronomy for Exoplanet Candidate Identification    5\n",
       "Running Python or R in Android OS                                                 5\n",
       "                                                                                  1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series([md_header_split.metadata.get('Header 2', '') for md_header_split in md_header_splits]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9311c0-8107-4c88-a659-2ab2cd948abf",
   "metadata": {},
   "source": [
    "To fix the chunking issue the `---` header section had to be added to each .md file. Now things look better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1356f553-8513-4726-854a-3b5b8ffa5531",
   "metadata": {},
   "source": [
    "## Using SpacyEmbeddings and retrieving using np.dot similarity without using Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "addb32ac-ef71-4d6f-8c84-908d2a19a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "embedding = SpacyEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fda25ae-e43e-45bc-b55e-6baae24d110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding.embed_query(\"What's my laptop configuration?\")\n",
    "embedding2s = [embedding.embed_query(md_header_split.page_content) for md_header_split in md_header_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25995398-33fc-4469-807b-9c7cc4dade24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"I'm currently pursuing independent research on large financial data. Data engineering is an integral part of the analysis as financial data is often not in the format required for analysis. Financial data may be provided in the form of transactions/updates or in the form of aggregates. While it is easy to go from transaction level to aggregate level, it is difficult to do the opposite.  \\nMy laptop configuration is decent: Ubuntu 18.04, 16 GB DDR4, Intel core i7–8750H (6 + 6 virtual core) @ 2.2 GHz and a GPU (not relevant here). But brute force is not a good idea — code run time was ~ 4 hours for processing one day's data, which is not acceptable.\",\n",
       "       '---\\nlayout: post\\ndate: 2019-01-11 12:00:00 -0500\\nimage: ../data/multiprocessing.jpg\\n---',\n",
       "       \"I recently extended the analysis, but there are several more ideas for 'discretization'.\\n---\\nlayout: post\\ndate: 2019-09-21 12:00:00 -0500\\nimage: ../data/Transit_photometry.gif\\n---\",\n",
       "       \"1.Follow the instructions in [this README] (https://github.com/Neo-Oli/termux-ubuntu/blob/master/README.md)\\n2. Run ./start-ubuntu. You're now a root user\\n3. Follow the normal steps used to install R on Ubuntu (similar steps for Python)  \\nI'm not sure if F-Droid can be uninstalled after this step. I will update this article once I'm in a position to experiment.\",\n",
       "       \"Summer internship can be tiring, but I feel the opposite. In the end, the ‘result’ is a binary flag which currently has a value False. This is one of the short run motivations. The long run motivation is philosophical. I rejected 2 (higher paying) offers to signup for a dream — to accomplish something significant in life. Dreams don't come true in 3 months, but there were significant learnings during Summer.\"],\n",
       "      dtype='<U2192')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "similarities = [np.dot(embedding2, embedding1) for embedding2 in embedding2s]\n",
    "top_k = np.argsort(-np.array(similarities))[0:k]\n",
    "np.array([md_header_split.page_content for md_header_split in md_header_splits])[top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed55b33-b5c8-443d-9e8c-b62289afa868",
   "metadata": {},
   "source": [
    "## Storing in vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "690e7989-1de5-4b40-aeef-ced57c16a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'docs/tech_non_tech_blog/'# Better to use a new folder here. There's a chromadb version compatibility issue while appending to the existing docs/chromadb folder\n",
    "vectordb = Chroma.from_documents(docs, embedding, persist_directory=persist_directory)\n",
    "if len(vectordb.get()['documents']) == 0:\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=md_header_splits,\n",
    "        embedding=embedding,# Reusing the spacy embedding\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934c956-0004-4b8f-b960-bf4078b3b145",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "### Using similarity_search (same as np.dot?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc069e05-4108-4201-89fe-8116d83a39cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm currently pursuing independent research on large financial data. Data engineering is an integral part of the analysis as financial data is often not in the format required for analysis. Financial data may be provided in the form of transactions/updates or in the form of aggregates. While it is easy to go from transaction level to aggregate level, it is difficult to do the opposite.  \\nMy laptop configuration is decent: Ubuntu 18.04, 16 GB DDR4, Intel core i7–8750H (6 + 6 virtual core) @ 2.2 GHz and a GPU (not relevant here). But brute force is not a good idea — code run time was ~ 4 hours for processing one day's data, which is not acceptable.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = [\"What's my laptop configuration?\", \"How long did brute force run for?\"]\n",
    "ret_docs = [vectordb.similarity_search(question, k=3) for question in questions]\n",
    "ret_docs[0][0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00130522-9df5-4f90-aa67-9277db502f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. Brushing, zooming and refining are time consuming tasks. They are needed because the current set of methods are not robust to 'noisy tagging' — for example: terminating the task after brushing (without zooming and refining) leads to light curve of candidate + default light curve of the host star.\\n2. Can this be simplified further? Sections of the image can be automatically brushing (using unsupervised learning) and labeled as candidates. This process can be repeated for all light curves. It is alright to have few false positives, but the algorithm should produce much fewer false negatives. Manual taggers only have to click on the 'x' button on each brush or leave the light curve unaltered (approval that the automatic candidate identification is accurate). Brushing, zooming and refining will still be available to taggers, but the hope here is to reduce their usage.  \\n- If a light curve has 'large' number of taggers, their approval and disapproval (+ optional refining) can be used to denoise the zone in which the candidate is present\\n- If the light curve has 'small' number of taggers, the default set of candidates tagged by the algorithm may be considered  \\nTherefore, unsupervised learning can expedite the process of candidate identification. Eventually each 'zone' of the light curve and each user who tags will be given a credibility score. This score varies with each newly available example of 'modified' manual tagging. Eventually, the most credible candidates can be examined further or can be used to build a supervised learning model.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_docs[1][0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc742f9-6c62-4827-84e0-7a65ae00b833",
   "metadata": {},
   "source": [
    "## Using max_marginal_relevance_search\n",
    "\n",
    "Maximum marginal relevance yields a different ordering of the retrieved documents compared to similarity_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e30cbe-608b-4e85-bb5c-e5abbf264d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"I'm currently pursuing independent research on large financial data. Data engineering is an integral part of the analysis as financial data is often not in the format required for analysis. Financial data may be provided in the form of transactions/updates or in the form of aggregates. While it is easy to go from transaction level to aggregate level, it is difficult to do the opposite.  \\nMy laptop configuration is decent: Ubuntu 18.04, 16 GB DDR4, Intel core i7–8750H (6 + 6 virtual core) @ 2.2 GHz and a GPU (not relevant here). But brute force is not a good idea — code run time was ~ 4 hours for processing one day's data, which is not acceptable.\", metadata={'Header 2': \"Speeding-up 'diff' between Consecutive Rows in Python on My Laptop\", 'Header 3': 'Introduction'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr = [vectordb.max_marginal_relevance_search(question, k=3) for question in questions]\n",
    "docs_mmr[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d2b0e65-89dc-4a70-9593-36821e095b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='If the transition probabilities are known, a model free approximation of the Q function can be computed based on historical data. The agent can be made to act in a greedy way (immediate reward, without exploration) to achieve its goal. However, it should be noted that a set of locally optimal paths is not guaranteed to build a globally optimal path. Therefore, this method is not guaranteed to work in environments that are drastically different from those observed in the past.  \\nIf the transition probabilities are known, a model free approximation of the Q function can be computed based on historical data. The agent can be made to act in a greedy way (immediate reward, without exploration) to achieve its goal. However, it should be noted that a set of locally optimal paths is not guaranteed to build a globally optimal path. Therefore, this method is not guaranteed to work in environments that are drastically different from those observed in the past.', metadata={'Header 2': 'Lessons from my Internship (and Immediate Aftermath)', 'Header 3': 'Simpler Solution'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b991bd8-dcb6-42d6-bc3f-c76ce9a47716",
   "metadata": {},
   "source": [
    "#### Narrowing the search space by applying metadata filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd3139f0-f99f-4a64-8199-4b5c502162ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"I'm currently pursuing independent research on large financial data. Data engineering is an integral part of the analysis as financial data is often not in the format required for analysis. Financial data may be provided in the form of transactions/updates or in the form of aggregates. While it is easy to go from transaction level to aggregate level, it is difficult to do the opposite.  \\nMy laptop configuration is decent: Ubuntu 18.04, 16 GB DDR4, Intel core i7–8750H (6 + 6 virtual core) @ 2.2 GHz and a GPU (not relevant here). But brute force is not a good idea — code run time was ~ 4 hours for processing one day's data, which is not acceptable.\", metadata={'Header 2': \"Speeding-up 'diff' between Consecutive Rows in Python on My Laptop\", 'Header 3': 'Introduction'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_docs = [vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={\"Header 2\":\"Speeding-up 'diff' between Consecutive Rows in Python on My Laptop\"}\n",
    ") for question in questions]\n",
    "ret_docs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5d7b047-413b-4f57-9a7c-c7dcf39ee291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='#### Problem  \\nThe result is a pandas series of lists. But my desired output is a structure with timestamp + edit in each row. So I used pd.concat(result), which had an additional run time of 3.5 mins (~ clock time for consecutive diff which is a relatively complicated process). This drove me nuts!  \\n#### Solution  \\nI found that np.concatenate is much faster (~ 40 sec) because of homogeneity. For simplicity I converted timestamp and all col_idx_values to string. The output was written to a CSV file using np.savetxt(\"file.csv\", concat_result, fmt=\"%s\") or using array_name.tofile(\"file.csv\", sep=\",\", format=\"%s\").', metadata={'Header 2': \"Speeding-up 'diff' between Consecutive Rows in Python on My Laptop\", 'Header 3': 'Concatenation — An Additional Problem'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_docs[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1875385-ae0d-416f-b2e9-7cbd07e77433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"I strongly recommend people to support [Planet Hunter TESS](https://www.zooniverse.org/projects/nora-dot-eisner/planet-hunters-tess) and other [Zooniverse](https://www.zooniverse.org/projects) projects. Many projects require support from a large pool of enthusiastic individuals (like the readers who have reached this section). Crowdsourcing in scientific research will grow exponentially and I recommend the readers to be one of the early members of this growing community.  \\nThe previous section of article presented the vision for my independent research — unsupervised learning and crowdsourcing for exoplanet candidate identification. I'm currently developing the idea on my own, so progress has been slow. I welcome people to join me.  \\n**Note:** This project will always remain open source.\\n---\\nlayout: post\\ndate: 2021-10-31 12:00:00 -0500\\nimage: ../data/R_troll.jpg\\n---\", metadata={'Header 2': 'Search for Exoplanets - Humans vs. Stars', 'Header 3': 'Conclusion'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_docs = [vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={\"Header 2\":\"Search for Exoplanets - Humans vs. Stars\"}\n",
    ") for question in questions]\n",
    "ret_docs[0][0]# Shouldn't be very relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12c89b3c-16c0-4a60-ac4a-36fa3ba878ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"1. Brushing, zooming and refining are time consuming tasks. They are needed because the current set of methods are not robust to 'noisy tagging' — for example: terminating the task after brushing (without zooming and refining) leads to light curve of candidate + default light curve of the host star.\\n2. Can this be simplified further? Sections of the image can be automatically brushing (using unsupervised learning) and labeled as candidates. This process can be repeated for all light curves. It is alright to have few false positives, but the algorithm should produce much fewer false negatives. Manual taggers only have to click on the 'x' button on each brush or leave the light curve unaltered (approval that the automatic candidate identification is accurate). Brushing, zooming and refining will still be available to taggers, but the hope here is to reduce their usage.  \\n- If a light curve has 'large' number of taggers, their approval and disapproval (+ optional refining) can be used to denoise the zone in which the candidate is present\\n- If the light curve has 'small' number of taggers, the default set of candidates tagged by the algorithm may be considered  \\nTherefore, unsupervised learning can expedite the process of candidate identification. Eventually each 'zone' of the light curve and each user who tags will be given a credibility score. This score varies with each newly available example of 'modified' manual tagging. Eventually, the most credible candidates can be examined further or can be used to build a supervised learning model.\", metadata={'Header 2': 'Search for Exoplanets - Humans vs. Stars', 'Header 3': 'Can this task be simplified?'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_docs[1][0]# Shouldn't be very relevant, but there's some discussion on brute force approach to exoplanet detection using crowd-sourcing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5cedd6-ce20-4616-a317-c227a037a294",
   "metadata": {},
   "source": [
    "### Other types of retrievers\n",
    "\n",
    "Using the in-memory document chunks; not the chunks and metadata stored in Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0de65c80-86aa-4955-a151-4f5f5e39344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "svm_retriever = SVMRetriever.from_texts([md_header_split.page_content for md_header_split in md_header_splits], embedding)\n",
    "tfidf_retriever = TFIDFRetriever.from_texts([md_header_split.page_content for md_header_split in md_header_splits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8541d02a-410b-4436-975f-e1bd3d7870ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"I'm currently pursuing independent research on large financial data. Data engineering is an integral part of the analysis as financial data is often not in the format required for analysis. Financial data may be provided in the form of transactions/updates or in the form of aggregates. While it is easy to go from transaction level to aggregate level, it is difficult to do the opposite.  \\nMy laptop configuration is decent: Ubuntu 18.04, 16 GB DDR4, Intel core i7–8750H (6 + 6 virtual core) @ 2.2 GHz and a GPU (not relevant here). But brute force is not a good idea — code run time was ~ 4 hours for processing one day's data, which is not acceptable.\")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_svm = [svm_retriever.get_relevant_documents(question) for question in questions]\n",
    "docs_svm[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3449a7f8-6563-4a3d-858f-2239ff55dd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='If the transition probabilities are known, a model free approximation of the Q function can be computed based on historical data. The agent can be made to act in a greedy way (immediate reward, without exploration) to achieve its goal. However, it should be noted that a set of locally optimal paths is not guaranteed to build a globally optimal path. Therefore, this method is not guaranteed to work in environments that are drastically different from those observed in the past.  \\nIf the transition probabilities are known, a model free approximation of the Q function can be computed based on historical data. The agent can be made to act in a greedy way (immediate reward, without exploration) to achieve its goal. However, it should be noted that a set of locally optimal paths is not guaranteed to build a globally optimal path. Therefore, this method is not guaranteed to work in environments that are drastically different from those observed in the past.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_svm[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dcb62dc-1389-4cec-a5a7-92aad5ee2b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"I'm currently pursuing independent research on large financial data. Data engineering is an integral part of the analysis as financial data is often not in the format required for analysis. Financial data may be provided in the form of transactions/updates or in the form of aggregates. While it is easy to go from transaction level to aggregate level, it is difficult to do the opposite.  \\nMy laptop configuration is decent: Ubuntu 18.04, 16 GB DDR4, Intel core i7–8750H (6 + 6 virtual core) @ 2.2 GHz and a GPU (not relevant here). But brute force is not a good idea — code run time was ~ 4 hours for processing one day's data, which is not acceptable.\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_tfidf = [tfidf_retriever.get_relevant_documents(question) for question in questions]\n",
    "docs_tfidf[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66723678-6b06-4375-b496-0cc0f3e5c119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Disclaimer: If one has the option, one should use rstudio.cloud and shinyapps.io to develop and deploy RShiny apps. Following this article is a waste of time.  \\nWorking on R/Python in Android OS may sound like an artificial constraint, but I faced this scenario - I wanted to run a RShiny app, but I did not have the resources to run apps in the 'free tier' both locally and on the Cloud.  \\nExperts claim Android OS is just a version of Linux, but I'm not an expert in this area. This short article covers how a layman (like me) can run R scripts in Android OS. Similar steps can be followed to install and run Python in Android OS. Clearly, this is not an everyday scenario; not even something worth trying for fun.\")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_tfidf[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df528c7e-73d4-41fa-b3d0-f99deb99da3c",
   "metadata": {},
   "source": [
    "## Question answering\n",
    "\n",
    "Note: For now this section of code won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32c1bfcc-d817-4a34-a7ae-1a1e987653c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = open(\"../OPENAI_API_KEY.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76c55a7c-da11-4744-b25c-71033b97e612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "current_date = datetime.datetime.now().date()\n",
    "if current_date < datetime.date(2023, 9, 2):\n",
    "    llm_name = \"gpt-3.5-turbo-0301\"\n",
    "else:\n",
    "    llm_name = \"gpt-3.5-turbo\"\n",
    "print(llm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c973410-8d9b-4775-9c94-eafafb9ae4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0694cf0-a4a1-4254-bdc7-c97bf1a8fa05",
   "metadata": {},
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60141ec3-6495-4331-bf2a-0544badf0b63",
   "metadata": {},
   "source": [
    "### Using langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd4e308e-82bc-4859-95f4-306fa42c1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = open(\"../LANGCHAIN_API_KEY.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "510405d1-36ad-4b8d-84c2-0dd45c673086",
   "metadata": {},
   "source": [
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"refine\"# can use \"map_reduce\" or \"refine\"\n",
    ")\n",
    "result = qa_chain_mr({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d69afe-96ca-4ff5-92f1-e2e4af2b9305",
   "metadata": {},
   "source": [
    "## Chat\n",
    "\n",
    "Note: For now this section of code won't work."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d194047f-c7f3-4155-87e0-97c572413f1b",
   "metadata": {},
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d554d2-da1b-4ad8-a987-db9892c17a4a",
   "metadata": {},
   "source": [
    "### Memory in chat"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eede93bd-29d4-4dfb-adb5-c3c9f82b69b4",
   "metadata": {},
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "retriever=vectordb.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "result = qa({\"question\": question})\n",
    "result['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dlai-short-courses)",
   "language": "python",
   "name": "dlai-short-courses"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
